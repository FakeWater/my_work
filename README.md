# my_work
常用算法通俗整理（有监督篇）

1 逻辑回归(Logistic Regression)：将线性回归的结果通过sigmoid函数映射到了(0,1)区间，可以当做是正例的概率，从而实现分类。不能实现回归。


2 决策树(Decision Tree)：决策树是一种递归的选择最优特征，然后根据最优特征分割数据集，使得各个子集都有一个最好的分类。回归则是通过最小二乘法来实现。不同的算法有不同的分类依据：

算法	ID3	C4.5	CART
分类依据	信息增益	信息增益率	基尼(gini)
3 K近邻(K-Nearest Neighbor)：用距离该样本最近的K(奇数)个点的标签投票，得到该样本的标签。回归则是使用最近K个点的均值。

4 朴素贝叶斯(Naive Bayes)：前提假设特征之间相互独立，由训练集可以得到X的先验概率P(X)，y的先验概率P(Y)，已知y发生的情况下x的条件概率P(X|Y)。就可以通过贝叶斯公式：P(Y|X)=P(Y)*P(X|Y) / P(X) 计算出已知X发生的情况下Y的条件概率从而实现分类，朴素贝叶斯不能实现回归。

5 支持向量机(Support Vector Machine)：
……1）分类(SVC)：是寻找一个最优分割平面将样本分开。线性可分是指分割面将样本完全干净分类，而有时候完全分类并不是整体的最优分割方式，于是就有了线性支持，可以容忍个别点在分割面的另一侧。线性不可分的时候，需要引入核函数，将低维不可分样本映射至新的高维特征空间，实现核空间可分。相当于一张纸上正例样本把负例样本围了一圈，这个时候就是线性不可分，引入核函数映射至高维相当于把正例从纸面上拔出来，把负例压进去，这样就可以实现核空间可分了，但也更容易过拟合。常用的核函数：效果由弱至强。

线性核	sigmoid核	多项式核	高斯核
‘linear’	‘sigmoid’	‘poly’	‘rbf’
……2）回归(SVR)：其实是我们假定预测值f(x)与真实值y之间的差距的绝对值在ε之内是可以接受的，于是就有了一个可容忍误差的间隔带，落在间隔带中的样本不计算损失。


=====================我是分割线，以上是常用的有监督基学习器，下面说集成模型

6 集成学习之装袋法(Bagging：bootstrap aggregating)：对样本进行有放回的抽样m次(可并行)，构建m个基(弱)学习器（之间相互独立），通过这m个基学习器进行投票(实现分类)，或者算术平均(实现回归)。这种集成方式本身就比较注重减小方差，因此每个基学习器的目标就是减小偏差（更好的拟合，甚至接近过拟合），比如采用深度较大的甚至不剪枝的树。

7 随机森林(Random Forest)：随机森林就是典型的bagging，并且在此基础上，对样本进行有放回的抽样同时，也对特征也进行有放回的抽样，进一步增加了模型的泛化能力。

8 集成学习之提升法(boosting)：boosting经常会被拿来与bagging进行比较，与bagging不同的是，boosting是一种迭代算法，它可以将弱学习器提升为强学习器。每一次的迭代会依赖于上一次迭代的结果(串行)，在迭代过程中不断加强识别上一次训练分错样本的学习比重，也就是增加分错样本的权重，降低分对样本的权重(被抽中的概率)。可以看出这种集成方式本身比较关注降低偏差，因此在不断的迭代过程中实现最终的强化学习。基学习器学习完成后，进行组合，正确率高的基学习器的权重会高一些。

9 Adaboost：Adaboost就是boosting的一个典型应用。

10 GBDT(Gradient Boosting Decision Tree)：GBDT也是一种boosting算法，但不同于adaboost，GBDT是通过计算损失函数的梯度，沿着负梯度方向优化，以得到最小化的损失函数。GBDT的基学习器是分类回归树(CART)中的回归树(RT)，这里就会发出一个疑问，既然基学习器是回归树，那么GBDT就只能解决回归问题？但其实我们知道GBDT也能解决分类问题，只不过GBDT在解决分类问题的时候其实是基于概率的回归。GBDT的损失函数比较常用均方误差，此时损失函数的梯度正好是残差，因此很多人也说GBDT的优化目标是残差。但是均方误差会放大异常值的影响，因此为增加模型的鲁棒性，也常用绝对误差和鲁棒误差作为损失函数。

11 XGboost：从名字就可以看出来陈天奇等人开发的xgb也是个boosting，大家一般也经常会拿xgb与GBDT进行比较：
……1）基学习器：GBDT的基学习器仅支持RT，xgb还支持线性学习器。
……2）损失函数：xgb利用了损失函数的二阶导数，从而能够更快的收敛，并且在损失函数中加入了正则项，限制了每颗树的复杂度，防止过拟合。
……3）列抽样：xgb借鉴了随机森林的列抽样，在建树之前先列抽样，然后分裂时只遍历这些特征，增加模型的泛化能力。
……4）支持缺失值：对于输入的稀疏特征(特征中缺失值或0值较多)，通过稀疏感知算法，学习处理缺失值的最佳方向。
……5）支持并行：因为boosting这种迭代模型的并行和bagging不同，首先树与树之间是串行的，但同层节点可并行，候选分裂节点增益计算时采用多线程并行，提高计算速度。

12 LightGBM：微软在2017年开源的lgb也是一种boosting算法，lgb最大的特点如下：
……1）梯度单边采样(GOSS:Gradient one-side Sampling)：先对所有样本进行梯度排序，保留贡献更多信息增益的大梯度样本，剩下的小梯度样本进行随机采样，模型训练更加高效。
……2）互斥特征捆绑(EFB:Exclusive Feature Bundling)：将稀疏的互斥特征进行捆绑来实现一定程度上的降维。打个比方，特征X1的取值范围是[0,10)，特征X2的取值范围是[10,20]，这两个稀疏特征是互斥的(也就是说取值范围不冲突)，这样就可以把这两个特征进行捆绑，合成为一个取值范围[0,20]的特征。
……3）直方图优化：在训练前预先把特征值转化为bin，也就是对每个特征的取值做个分段函数，将所有样本在该特征上的取值划分到某一段（bin）中，其实就是提前对连续变量做了一个分箱离散化处理，然后统计每个bin中的样本梯度之和与样本个数。这样做的好处是可以极大的降低了存储空间的使用，并且也大幅降低了计算代价。虽然降低了分割精度，但在梯度提升框架中对结果的影响不大，并且这样做还能起到一定的正则化效果。
……4）支持类别型变量：通常我们喂给模型的类别型变量都是经过哑变量编码或者是独热编码的，也正是由于直方图法的优化，lgb可以让我们省去了对类别型变量进行编码的特征预处理。
……5）深度限制节点(leaf-wise)展开法：不同于xgb的level-wise(按层展开，每层的节点数量一样多)，leaf-wise优先选择更好的节点进行展开。这就会导致树会长得不平衡，也就是说当节点数一样多的时候，leaf-wise的树会更深，这就意味着leaf-wise更精确，但同时也更容易或拟合，所以要加入深度限制。

